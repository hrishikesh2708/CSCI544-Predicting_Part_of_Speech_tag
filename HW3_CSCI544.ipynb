{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/l_cy7_tj5pn7bp93mtmcrhg80000gn/T/ipykernel_7648/2656356677.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths \n",
    "train_file = \"./data/train\"\n",
    "dev_file = \"./data/dev\"\n",
    "test_file = \"./data/test\"\n",
    "\n",
    "def dataset_preparation(file_path, columnnames):\n",
    "    dataset = pd.read_csv(file_path, sep=\"\\t\", header=None, names=columnnames)    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = dataset_preparation(train_file, columnnames=[\"index\", \"word\", \"pos_tags\" ])\n",
    "dev_dataset = dataset_preparation(dev_file, columnnames=[\"index\", \"word\", \"pos_tags\" ])\n",
    "test_dataset = dataset_preparation(test_file, columnnames = [\"index\", \"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Creation\n",
    "threshold = 2\n",
    "vocab_count = train_dataset.word.value_counts() \n",
    "handle_rare = pd.DataFrame({\"word\": vocab_count.index, \"occurrences\" : vocab_count.values})\n",
    "handle_rare.loc[handle_rare.occurrences < threshold, \"word\"] = \"<unk>\"\n",
    "count_unk = handle_rare.word.value_counts()\n",
    "word = count_unk.index.to_list()\n",
    "occurrence = [ vocab_count[w] if w != \"<unk>\" else count_unk[w] for w in word ]\n",
    "vocabulary = pd.DataFrame({\"word\": word, \"occurrences\" : occurrence})\n",
    "vocabulary = pd.concat([vocabulary.loc[:0],vocabulary.loc[1:].sort_values(by=[\"occurrences\"], ascending=False)], ignore_index=True)\n",
    "vocabulary[\"index\"] = [ i for i in range( vocabulary.shape[0])]\n",
    "vocabulary = vocabulary[[\"word\", \"index\", \"occurrences\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size of my vocabulary (including <unk>) : 23182\n",
      "Total occurances of the special token <unk> : 20011\n"
     ]
    }
   ],
   "source": [
    "# What is the selected threshold for unknown words replacement? What is the total size of your vocabulary ?\n",
    "print( \"Total Size of my vocabulary (including <unk>) :\", vocabulary.shape[0])\n",
    "\n",
    "# what is the total occurrences of the special token ‘< unk >’ after replacement?\n",
    "print( \"Total occurances of the special token <unk> :\", vocabulary.loc[0].occurrences)\n",
    "\n",
    "# save vocabulary to vocab.txt file\n",
    "vocabulary.to_csv(\"vocab.txt\", sep=\"\\t\", index=None, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique tags and their occurances\n",
    "unique_tags_count = train_dataset.pos_tags.value_counts()\n",
    "tags_set = unique_tags_count.index\n",
    "tags = train_dataset.pos_tags\n",
    "transition = {tag: {t: 0 for t in tags_set} for tag in tags_set}\n",
    "initial_transition = train_dataset[train_dataset[\"index\"] == 1].pos_tags.to_list()\n",
    "transition[\"<start>\"]= {t: 0 for t in tags_set}\n",
    "for tag in initial_transition:\n",
    "    transition[\"<start>\"][tag] += 1\n",
    "    \n",
    "for index in range(1, len(tags)):\n",
    "    transition[tags[index-1]][tags[index]] += 1\n",
    "    \n",
    "for tag in transition.keys():\n",
    "    tag_count = unique_tags_count[tag] if tag != \"<start>\" else len(initial_transition)\n",
    "    for t in transition[tag].keys():\n",
    "        if  transition[tag][t] == 0:\n",
    "            transition[tag][t] = 1e-10\n",
    "        else:\n",
    "            transition[tag][t] /= tag_count\n",
    "\n",
    "transition_object = {}\n",
    "for tag in transition.keys():\n",
    "    for t in transition[tag].keys():\n",
    "        if transition[tag][t] != 0:\n",
    "            transition_object[f\"({tag},{t})\"] = transition[tag][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary \n",
    "vocab_with_pos = []\n",
    "for index in range(train_dataset.shape[0]):\n",
    "    data = train_dataset.loc[index]\n",
    "    if data.word in vocabulary.word.to_list():\n",
    "        vocab_with_pos.append([data.word, data.pos_tags])\n",
    "    else:\n",
    "        vocab_with_pos.append([\"<unk>\", data.pos_tags])\n",
    "\n",
    "emission = {tag: {word: 0 for word in vocabulary.word.to_list()} for tag in tags_set}\n",
    "\n",
    "for word, pos_tag in vocab_with_pos:\n",
    "    emission[pos_tag][word] += 1\n",
    "\n",
    "for tag in emission.keys():\n",
    "    for word in emission[tag].keys():\n",
    "        if emission[tag][word] == 0:\n",
    "            emission[tag][word] = 1e-10\n",
    "        else:\n",
    "            tag_count = unique_tags_count[tag]\n",
    "            emission[tag][word] /= tag_count\n",
    "        \n",
    "# creating emission object\n",
    "emission_object = {}\n",
    "for tag in emission.keys():\n",
    "    for word in emission[tag].keys():\n",
    "        if emission[tag][word]:\n",
    "            emission_object[f\"({tag},{word})\"] = emission[tag][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Transition parameters : 2070\n",
      "Total number of emission parameters : 1043190\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of Transition parameters :\", len(transition_object))\n",
    "print(\"Total number of emission parameters :\", len(emission_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hmm.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump( {\"transition\": transition_object, \"emission\": emission_object} , file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(sentences, tags_set = tags_set, transition = transition, transition_object= transition_object,\n",
    "                    emission_object = emission_object):\n",
    "    tagPredictions = []\n",
    "    tagScores = []\n",
    "    for sent in sentences:\n",
    "        previousTag = \"<start>\"\n",
    "        sentPrediction = []\n",
    "        sentScore = []\n",
    "        for i in range(len(sent)):\n",
    "            highestScore = -1\n",
    "            \n",
    "            for j in range(len(tags_set)):\n",
    "                currScore = 1\n",
    "                if i == 0:\n",
    "                    currScore *= transition[\"<start>\"][tags_set[j]]\n",
    "                else:\n",
    "                    if str(\"(\" + previousTag + \",\" + tags_set[j] + \")\") in transition_object:\n",
    "                        currScore *= transition_object[\"(\" + previousTag + \",\" + tags_set[j] + \")\"]\n",
    "                if str(\"(\" + tags_set[j] + \",\" + str(sent[i]) + \")\") in emission_object:\n",
    "                    currScore *= emission_object[\"(\" + tags_set[j] + \",\" + str(sent[i]) + \")\"]\n",
    "                else:\n",
    "                    currScore *= emission_object[\"(\" + tags_set[j] + \",\" + \"<unk>\" + \")\"]\n",
    "                if(currScore > highestScore):\n",
    "                    highestScore = currScore\n",
    "                    highestProbabilityTag = tags_set[j]\n",
    "            \n",
    "            previousTag = highestProbabilityTag\n",
    "            sentPrediction.append(previousTag)\n",
    "            sentScore.append(highestScore)\n",
    "\n",
    "        tagPredictions.append(sentPrediction)\n",
    "        tagScores.append(sentScore)\n",
    "    \n",
    "    return tagPredictions, tagScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = []\n",
    "dev_sentence = []\n",
    "for index, row in dev_dataset.iterrows():\n",
    "    if row[\"index\"] == 1:\n",
    "        dev_sentences.append(dev_sentence)\n",
    "        dev_sentence = []\n",
    "        \n",
    "    dev_sentence.append(row[\"word\"])\n",
    "dev_sentences.append(dev_sentence)\n",
    "\n",
    "prediction, scores  = greedy_decoding(dev_sentences)\n",
    "flatten_prediction = [j for sub in prediction for j in sub]\n",
    "dev_dataset[\"prediction\"] = flatten_prediction\n",
    "dev_dataset[[\"word\",\"index\",\"prediction\"]].to_csv(\"dev_greedy.out\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 123218, accuracy: 93.51%\n"
     ]
    }
   ],
   "source": [
    "! python eval.py -p ./dev_greedy.out -g ./data/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "test_sentence = []\n",
    "for index, row in test_dataset.iterrows():\n",
    "    if row[\"index\"] == 1:\n",
    "        test_sentences.append(test_sentence)\n",
    "        test_sentence = []        \n",
    "\n",
    "        \n",
    "    test_sentence.append(row[\"word\"])\n",
    "test_sentences.append(test_sentence)\n",
    "\n",
    "prediction, scores  = greedy_decoding(test_sentences)\n",
    "flatten_prediction = [j for sub in prediction for j in sub]\n",
    "test_dataset[\"prediction\"] = flatten_prediction\n",
    "test_dataset[[\"word\",\"index\",\"prediction\"]].to_csv(\"greedy.out\", sep=\"\\t\", header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python eval.py -p ./greedy.out -g ./test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(sentence, tags_set = tags_set, transition = transition, transition_object= transition_object,\n",
    "                    emission_object = emission_object):\n",
    "    n = len(tags_set)\n",
    "    viterbi_list = []\n",
    "    data = {}\n",
    "    for t in tags_set:\n",
    "        if \"(\" + t + \",\" + str(sentence[0]) + \")\" in emission_object:\n",
    "            viterbi_list.append(transition[\"<start>\"][t] * emission_object[\"(\" + t + \",\" + str(sentence[0]) + \")\"])\n",
    "        else:\n",
    "            # print(\" mein ja raha hu mujhe yeh nahi mill raha hai\" , \"(\" + t + \",\" + str(sentence[0]) + \")\" )\n",
    "            viterbi_list.append(transition[\"<start>\"][t] * emission_object[\"(\" + t + \",\" + \"<unk>\" + \")\"])\n",
    "\n",
    "    for i,word in enumerate(sentence):\n",
    "        if i == 0: continue\n",
    "        temp_list = [None] * n\n",
    "        for j,tag in enumerate(tags_set):\n",
    "            \n",
    "            score = -1\n",
    "            val = 1\n",
    "            for k, prob in enumerate(viterbi_list):\n",
    "                if str(\"(\" + tags_set[k] + \",\" + tag + \")\") in transition_object and str(\"(\" + tag + \",\" + str(word) + \")\") in emission_object:\n",
    "                    val = prob * transition_object[\"(\" + tags_set[k] + \",\" + tag + \")\"] * emission_object[\"(\" + tag + \",\" + str(word) + \")\"]\n",
    "                else:\n",
    "                    # print(\" mein ja raha hu mujhe yeh nahi mill raha hai\" , str(\"(\" + tag + \",\" + str(word) + \")\"))                  \n",
    "                    val = prob * transition_object[\"(\" + tags_set[k] + \",\" + tag + \")\"] * emission_object[\"(\" + tag + \",\" + \"<unk>\" + \")\"]\n",
    "                if(score < val):\n",
    "                    score = val\n",
    "                    data[str(i) + \", \" + tag] = [tags_set[k], val]\n",
    "            temp_list[j] = score\n",
    "        viterbi_list = [x for x in temp_list]\n",
    "    \n",
    "    return data, viterbi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(tags_set, data, viterbi_list):\n",
    "    num_states = len(tags_set)\n",
    "    n = len(data) // num_states\n",
    "    best_sequence = []\n",
    "    best_sequence_breakdown = []\n",
    "    x = tags_set[np.argmax(np.asarray(viterbi_list))]\n",
    "    best_sequence.append(x)\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        val = data[str(i) + ', ' + x][1]\n",
    "        x = data[str(i) + ', ' + x][0]\n",
    "        best_sequence = [x] + best_sequence\n",
    "        best_sequence_breakdown =  [val] + best_sequence_breakdown\n",
    "    \n",
    "    return best_sequence, best_sequence_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "v = []\n",
    "for sentence in dev_sentences[1:]:\n",
    "    a, b = viterbi_decoding(sentence)\n",
    "    c.append(a)\n",
    "    v.append(b)\n",
    "\n",
    "best_seq = []\n",
    "best_seq_score = []\n",
    "for data, viterbi_list in zip(c, v):\n",
    "    a, b = viterbi_backward(tags_set, data, viterbi_list)\n",
    "    best_seq.append(a)\n",
    "    best_seq_score.append(b)\n",
    "\n",
    "flatten_prediction = [j for sub in best_seq for j in sub]\n",
    "len(flatten_prediction)\n",
    "dev_dataset[\"viterbi_prediction\"] = flatten_prediction\n",
    "dev_dataset[[\"word\",\"index\",\"viterbi_prediction\"]].to_csv(\"dev_viterbi.out\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 124924, accuracy: 94.81%\n"
     ]
    }
   ],
   "source": [
    "! python eval.py -p ./dev_viterbi.out -g ./data/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "v = []\n",
    "for sentence in test_sentences[1:]:\n",
    "    a, b = viterbi_decoding(sentence)\n",
    "    c.append(a)\n",
    "    v.append(b)\n",
    "\n",
    "best_seq = []\n",
    "best_seq_score = []\n",
    "for data, viterbi_list in zip(c, v):\n",
    "    a, b = viterbi_backward(tags_set, data, viterbi_list)\n",
    "    best_seq.append(a)\n",
    "    best_seq_score.append(b)\n",
    "\n",
    "flatten_prediction = [j for sub in best_seq for j in sub]\n",
    "test_dataset[\"viterbi_prediction\"] = flatten_prediction\n",
    "test_dataset[[\"word\",\"index\",\"viterbi_prediction\"]].to_csv(\"viterbi.out\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python eval.py -p ./viterbi.out -g ./data/dev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
